{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72ee685-d13f-4c15-b678-7389497c2923",
   "metadata": {},
   "source": [
    "Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3079d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.6.tar.gz (317.4 MB)\n",
      "\u001b[K     |███████████████████████████▊    | 274.6 MB 101.6 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 317.4 MB 1.1 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 80.9 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.6-py2.py3-none-any.whl size=317895818 sha256=8864011f58ef7a33de85bc859c6c17632c2add696c40481873e5d97ae9ec352a\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/59/22/7b/02883022a5a50e8e1a403bc1bacc4e9e8eb34b1699c09cd539\n",
      "Successfully built pyspark\n",
      "Installing collected packages: findspark, py4j, pyspark\n",
      "Successfully installed findspark-2.0.1 py4j-0.10.9.7 pyspark-3.5.6\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9400c-1adb-44dc-aaf7-e98eb28e611f",
   "metadata": {},
   "source": [
    "Конфигурация переменных для запуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0cb33d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accesskey=\"accesskey\"\n",
    "secretkey=\"secretkey\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b1dff-299f-4f46-9ad6-8fe2492e074e",
   "metadata": {},
   "source": [
    "Инициализация сессии спарк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbd66e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in ./.local/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: pyspark in ./.local/lib/python3.8/site-packages (3.5.6)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./.local/lib/python3.8/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769947e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4a4d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SOURCE_BUCKET=otus-bucket-b1g1p055ep53fi91o22r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Читаем имя бакета из переменной окружения\n",
    "SOURCE_BUCKET = os.getenv(\"SOURCE_BUCKET\", \"otus-bucket-default\")\n",
    "print(f\"Using SOURCE_BUCKET={SOURCE_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977f7a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TransactionValidation\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", accesskey) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secretkey) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"storage.yandexcloud.net\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437b4b5-e09d-4ec9-83d8-93f77779e4a8",
   "metadata": {},
   "source": [
    "Указание пути до данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbcace5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"s3a://data_path/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f5f39-0387-45c7-8d50-9bb22bfccc19",
   "metadata": {},
   "source": [
    "Чтение данных с облака:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2273a9c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"comment\", \"#\").option(\"timestampFormat\", \"ууyy-MM-dd HH:mm:ss\") \\\n",
    ".schema(\"transaction_id LONG, tx_datetime STRING, customer_id INT, terminal_id INT, tx_amount DOUBLE, tx_time_seconds LONG, tx_time_days LONG, tx_fraud INT, tx_fraud_scenario INT\") \\\n",
    ".csv('/user/ubuntu/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7968352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала ОБЯЗАТЕЛЬНО уменьшаем партиции\n",
    "df_optimized = df.coalesce(100)  # уменьшаем до 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558a2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика дубликатов:\n",
      "+--------------+-----+\n",
      "|transaction_id|count|\n",
      "+--------------+-----+\n",
      "|    1590802232|    2|\n",
      "|      42955533|    2|\n",
      "|    1024754394|    2|\n",
      "|    1057587324|    2|\n",
      "|    1692849430|    2|\n",
      "|    1549598224|    2|\n",
      "|    1553487795|    2|\n",
      "|     723885933|    2|\n",
      "|     659780807|    2|\n",
      "|     430593489|    2|\n",
      "|      26908859|    2|\n",
      "|     439107232|    2|\n",
      "|    1631616097|    2|\n",
      "|    1073430309|    2|\n",
      "|    1074286843|    2|\n",
      "|     811472993|    2|\n",
      "|     449649283|    2|\n",
      "|     683075166|    2|\n",
      "|     151617421|    2|\n",
      "|     432881584|    2|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Затем быстрый анализ на оптимизированном DF\n",
    "duplicate_stats = df_optimized.groupBy(\"transaction_id\") \\\n",
    "                             .count() \\\n",
    "                             .filter(\"count > 1\") \\\n",
    "                             .orderBy(\"count\", ascending=False)\n",
    "\n",
    "print(\"Статистика дубликатов:\")\n",
    "duplicate_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d307289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка данных\n",
    "df_clean = df_optimized.dropDuplicates(['transaction_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8725a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Валидация: все числовые поля должны быть >= 0\n",
    "from pyspark.sql.functions import col\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"transaction_id\") >= 0) &\n",
    "    (col(\"customer_id\") >= 0) &\n",
    "    (col(\"terminal_id\") >= 0) &\n",
    "    (col(\"tx_amount\") >= 0) &\n",
    "    (col(\"tx_time_seconds\") >= 0) &\n",
    "    (col(\"tx_time_days\") >= 0) &\n",
    "    (col(\"tx_fraud\") >= 0) &\n",
    "    (col(\"tx_fraud_scenario\") >= 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c493e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "# Преобразуем колонку в тип timestamp и одновременно отфильтруем некорректные значения\n",
    "df_clean = df_clean.withColumn(\"tx_datetime\", to_timestamp(\"tx_datetime\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "       .filter(col(\"tx_datetime\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем выбросы (IQR)\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    for col_name in columns:\n",
    "        q1, q3 = df.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        df = df.filter((col(col_name) >= lower_bound) & (col(col_name) <= upper_bound))\n",
    "    return df\n",
    "\n",
    "df_clean = remove_outliers_iqr(df_clean, [\"tx_amount\", \"tx_time_seconds\", \"tx_time_days\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем выбросы (IQR)\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    bounds = {}\n",
    "    for col_name in columns:\n",
    "        q1, q3 = df.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        bounds[col_name] = (lower_bound, upper_bound)\n",
    "\n",
    "    # Применяем все фильтры одновременно\n",
    "    filter_condition = None\n",
    "    for col_name, (low, high) in bounds.items():\n",
    "        cond = (col(col_name) >= low) & (col(col_name) <= high)\n",
    "        filter_condition = cond if filter_condition is None else filter_condition & cond\n",
    "\n",
    "    return df.filter(filter_condition)\n",
    "\n",
    "df_clean = remove_outliers_iqr(df_clean, [\"tx_amount\", \"tx_time_seconds\", \"tx_time_days\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просмотр выборосов\n",
    "df.exceptAll(remove_outliers_iqr(df,[\"customer_id\", \"terminal_id\", \"tx_time_seconds\",\"transaction_id\", \"tx_time_days\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "845de466",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=df_clean.orderBy(\"transaction_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9d823-9dce-4464-8aee-8e7ba9d7c0bf",
   "metadata": {},
   "source": [
    "Запись в bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "736cf332",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_write_path = f\"s3a://{SOURCE_BUCKET}/spark_output\"\n",
    "\n",
    "df.limit(40) \\\n",
    "  .write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .parquet(data_write_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc127d-d36c-4c68-be9e-3f6276778ddc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Остановка сессии spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "904cc552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
